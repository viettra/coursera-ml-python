{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Vectorizing Logistic Regression\n",
    "You will be using multiple one-vs-all logistic regression models to build a multi-class classifier. Since there are 10 classes, you will need to train 10 separate logistic regression classifiers. To make this training efficient, it is important to ensure that your code is well vectorized. In this section, you will implement a vectorized version of logistic regression that does not employ any `for` loops. \n",
    "\n",
    "#### 1.3.1 Vectorizing the cost function\n",
    "\n",
    "We will begin by writing a vectorized version of the cost function. Recall that in (unregularized) logistic regression, the cost function is:\n",
    "$$J(\\theta) = \\frac{1}{m}\\sum[-y^{(i)}\\log(h_\\theta(x^{(i)})-(1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]$$\n",
    "To compute each element in the summation, we have to compute $h_\\theta(x^{(i)})$ for every example $i$, where $h_\\theta(x^{(i)})= g(\\theta^Tx^{(i)})$ and $g(z)=\\frac{1}{1+e^{-z}}$ is the sigmoid function. It turns out that we can compute this quickly for all our examples by using matrix multiplication. Let us define $X$ and $\\theta$ as\n",
    "$$\n",
    "X=\\left[ \\begin{array}{c}\n",
    "-(x^{(1)})^T-  \\\\\n",
    "-(x^{(1)})^T-   \\\\\n",
    " \\vdots      \\\\\n",
    "-(x^{(m)})^T-   \\\\\n",
    " \\end{array} \\right]\n",
    "\\qquad and \\qquad \n",
    "\\theta = \\left[\\begin{array}{c}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n \\\\\n",
    "\\end{array} \\right].\n",
    "$$\n",
    "\n",
    "Then, by computing the matrix product $X\\theta$, we have\n",
    "\n",
    "$$\n",
    "X  \\theta = \\left[ \\begin{array}{c}\n",
    "-(x^{(1)})^T\\theta -  \\\\\n",
    "-(x^{(1)})^T\\theta -   \\\\\n",
    " \\vdots      \\\\\n",
    "-(x^{(m)})^T\\theta -   \\\\\n",
    " \\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c}\n",
    "-\\theta^T(x^{(1)})-  \\\\\n",
    "-\\theta^T(x^{(1)})-   \\\\\n",
    " \\vdots      \\\\\n",
    "-\\theta^T(x^{(m)})-   \\\\\n",
    " \\end{array} \\right].\n",
    "$$\n",
    "\n",
    "In the last equality, we used the fact that $a^Tb = b^Ta$ if $a$ and $b$ are vectors. This allows us to compute the products $\\theta^Tx(i)$θ T x (i) for all our examples i in one line of code.\n",
    "Your job is  to write the unregularized cost function in the file <b>lr_cost_function</b>. Your implementation should use the strategy we presented above to calculate $\\theta^Tx(i)$ . You should also use a vectorized approach for the rest of the cost function. A fully vectorized version of <b>lr_cost_function</b> should not contain any loops.\n",
    "#### 1.3.2 Vectorizing the gradient\n",
    "<div align=\"justify\"> <div style=\"text-indent: 25px\">Recall that the gradient of the (unregularized) logistic regression cost is a vector where the $j^{th}$ element is defined as</div></div>\n",
    "$$ \\frac {\\delta J}{\\delta \\theta_j} = \\frac{1}{m}\\sum^{m}_{i=1}{\\large((h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j)}$$\n",
    "\n",
    "To vectorize this operation over the dataset, we start by writing out all the partial derivatives explicitly for all $\\theta_j$,\n",
    "$$\n",
    "\\left[ \\begin{array}{c}\n",
    "    \\frac{\\delta J}{\\delta \\theta_0} \\\\\n",
    "    \\frac{\\delta J}{\\delta \\theta_1} \\\\\n",
    "    \\frac{\\delta J}{\\delta \\theta_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\delta J}{\\delta \\theta_n} \\\\\n",
    "\\end{array}\\right] = \\frac{1}{m}  \n",
    "\\left[ \\begin{array}{c}\n",
    "    \\sum^{m}_{i=1}{(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_0} \\\\\n",
    "    \\sum^{m}_{i=1}{(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_1} \\\\\n",
    "    \\sum^{m}_{i=1}{(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sum^{m}_{i=1}{(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_n} \\\\\n",
    "\\end{array}\\right]  \n",
    "    =\\frac{1}{m} \\sum^{m}_{i=1}{((h_\\theta(x^{(i)}) - y^{(i)}x^{(i)} )}  \n",
    "    =\\frac{1}{m} X^T(h_\\theta(x) - y). \n",
    "$$ \n",
    "\n",
    "where\n",
    "$\n",
    "h_\\theta(x) - y =\n",
    "\\left[\\begin{array}{c}\n",
    "    h_\\theta(x^{(i)}) - y^{(1)} \\\\ \n",
    "    h_\\theta(x^{(i)}) - y^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "    h_\\theta(x^{(i)}) - y^{(n)})\n",
    "\\end{array}\\right]\n",
    "$\n",
    "\n",
    "Note that $x^{(i)}$ is a vector, while $(h_\\theta(x^{(i)}) − y^{(i)})$ is a scalar (single number). To understand the last step of the derivation, let $\\beta_i = (h_\\theta(x^{(i)}) − y^{(i)})$ and observe that:\n",
    "$$\n",
    "\\sum_{i}{\\beta_ix^{(i)}} = \n",
    "    \\left[\\begin{array}{cccc}\n",
    "        \\vert &\\vert& &\\vert \\\\\n",
    "        x^{(1)} & x^{(2)} &\\cdots & x^{(m)}\\\\\n",
    "        \\vert &\\vert& & \\vert \n",
    "    \\end{array}\\right]  \n",
    "    \\left[\\begin{array}{c}\n",
    "        \\beta_1 \\\\\n",
    "        \\beta_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_m\n",
    "    \\end{array}\\right] = X^T\\beta, \\text{ where the values } \\beta_i = (h_\\theta(x^{(i)}) - y^{(i)}).\n",
    "$$\n",
    "The expression above allows us to compute all the partial derivatives without any loops. If you are comfortable with linear algebra, we encourage you to work through the matrix multiplications above to convince yourself that the vectorized version does the same computations. You should now implement above Equation to compute the correct vectorized gradient. Once you\n",
    "are done, complete the function <b>lr_cost_function</b> by implementing the gradient.\n",
    "<p style=\"border:3px; border-style:solid; border-color:#000000; padding: 1em;\">\n",
    "<b>Debugging Tip</b>: Vectorizing code can sometimes be tricky. One common strategy for debugging is to print out the sizes of the matrices you are working with using the size function. For example, given a data matrix $X$ of size $100 × 20$ (100 examples, 20 features) and $\\theta$, a vector with dimensions $20×1$, you can observe that $X\\theta$ is a valid multiplication operation, while $\\theta X$ is not. Furthermore, if you have a non-vectorized version of your code, you can compare the output of your vectorized code and non-vectorized code to make sure that they produce the same outputs.\n",
    "</p>  \n",
    "\n",
    "#### 1.3.3 Vectorizing regularized logistic regression\n",
    "After you have implemented vectorization for logistic regression, you will now add regularization to the cost function. Recall that for regularized logistic regression, the cost function is defined as \n",
    "$$J(\\theta) = \\frac{1}{m}\\sum^{m}_{i=1}{[-y^{(i)}\\log{(h_\\theta(x^{(i)}))} - (1 - y^{(i)})\\log{(1 - h_\\theta(x^{(i)}))} ]}\n",
    "+ \\frac{\\lambda}{2m} \\sum^{n}_{j=1}{\\theta^2_j}$$\n",
    "\n",
    "Note that you should not be regularizing $\\theta_0$ which is used for the bias term.  \n",
    "Correspondingly, the partial derivative of regularized logistic regression cost for $\\theta_j$ is defined as\n",
    "$$\n",
    "\\left.\\begin{array}{lr}\n",
    "    \\frac{\\delta J(\\theta)}{\\delta\\theta_0} = \\frac{1}{m}\\sum^{m}_{i=1}{(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j} & \n",
    "    \\text{, for } j=0 \\\\\n",
    "    \\frac{\\delta J(\\theta)}{\\delta\\theta_j} = (\\frac{1}{m}\\sum^{m}_{i=1}{(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j}) + \n",
    "    \\frac{\\delta}{m}\\theta_j & \\text{, for } j \\geq 1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Now modify your code in <b>lr_cost_function</b> to account for regularization.\n",
    "Once again, you should not put any loops into your code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
